{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fede411a-2aa3-433d-a39f-483073a5df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "URL = \"https://baraasalout.github.io/test.html\"\n",
    "page = requests.get(URL) #get page\n",
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4bb9f8f1-7c5f-432d-8555-b2583abe225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\") #parse the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ad95ce0c-5b08-40f7-a86d-76253d07785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Heading': 'Heading1', 'Content': 'Web Scraping Practice'}, {'Heading': 'Heading2', 'Content': 'Available Products'}, {'Heading': 'Heading2', 'Content': 'Product Table'}, {'Heading': 'Heading2', 'Content': 'Watch This Video'}, {'Heading': 'Heading2', 'Content': 'Contact Us'}, {'Heading': 'Heading2', 'Content': 'Product Information'}, {'Heading': 'Heading2', 'Content': 'Featured Products'}, {'Heading': 'Paragraph', 'Content': 'Welcome to the web scraping task! Use your skills to extract the required data from this page.'}, {'Heading': 'Paragraph', 'Content': 'Sharp Objects'}, {'Heading': 'Paragraph', 'Content': '£47.82'}, {'Heading': 'Paragraph', 'Content': '✔ In stock'}, {'Heading': 'Paragraph', 'Content': 'In a Dark, Dark Wood'}, {'Heading': 'Paragraph', 'Content': '£19.63'}, {'Heading': 'Paragraph', 'Content': '✔ In stock'}, {'Heading': 'Paragraph', 'Content': 'The Past Never Ends'}, {'Heading': 'Paragraph', 'Content': '£56.50'}, {'Heading': 'Paragraph', 'Content': '✔ In stock'}, {'Heading': 'Paragraph', 'Content': 'A Murder in Time'}, {'Heading': 'Paragraph', 'Content': '£16.64'}, {'Heading': 'Paragraph', 'Content': 'Out stock'}, {'Heading': 'Paragraph', 'Content': 'Wireless Headphones'}, {'Heading': 'Paragraph', 'Content': '$49.99'}, {'Heading': 'Paragraph', 'Content': 'Available colors: Black, White, Blue'}, {'Heading': 'Paragraph', 'Content': 'Smart Speaker'}, {'Heading': 'Paragraph', 'Content': '$89.99'}, {'Heading': 'Paragraph', 'Content': 'Available colors: Grey, Black'}, {'Heading': 'Paragraph', 'Content': 'Smart Watch'}, {'Heading': 'Paragraph', 'Content': '$149.99'}, {'Heading': 'Paragraph', 'Content': 'Available colors: Black, Silver, Gold'}, {'Heading': 'Paragraph', 'Content': '© 2024 Web Scraping Practice. All Rights Reserved.'}, {'Heading': 'Listitem', 'Content': 'Laptop'}, {'Heading': 'Listitem', 'Content': 'Smartphone'}, {'Heading': 'Listitem', 'Content': 'Tablet'}, {'Heading': 'Listitem', 'Content': 'Smartwatch'}]\n"
     ]
    }
   ],
   "source": [
    "x=[]#to have all elements with their values\n",
    "#get heading 1s\n",
    "for h in soup.find_all(\"h1\"):\n",
    "    x.append({\"Heading\": \"Heading1\", \"Content\": h.get_text().strip()})\n",
    "#get heading 2s\n",
    "for h in soup.find_all(\"h2\"):\n",
    "    x.append({\"Heading\": \"Heading2\", \"Content\": h.get_text().strip()})\n",
    "#get paragraphs\n",
    "for h in soup.find_all(\"p\"):\n",
    "    x.append({\"Heading\": \"Paragraph\", \"Content\": h.get_text().strip()})\n",
    "#get listitems\n",
    "for h in soup.find_all(\"li\"):\n",
    "    x.append({\"Heading\": \"Listitem\", \"Content\": h.get_text().strip()})\n",
    "#save as csv \n",
    "with open(\"Practice.csv\", mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Heading\", \"Content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(x)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cc6efff8-91a1-44f7-93b4-be4ca81e89e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product', 'Price', 'In Stock']\n",
      "['Laptop', '$1000', 'Yes', 'Smartphone', '$800', 'No', 'Tablet', '$500', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "table = [i.get_text() for i in soup.find_all(\"th\")]#get the headers\n",
    "td=[i.get_text() for i in soup.find_all(\"td\")]#get values\n",
    "print(table)\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "576e9fea-cb3d-4aca-bac2-5d9efc15a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Laptop', 'Smartphone', 'Tablet']\n",
      "['$1000', '$800', '$500']\n",
      "['Yes', 'No', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "product=[]\n",
    "price=[]\n",
    "instock=[]\n",
    "for i in range(0,len(td),3):\n",
    "    product.append(td[i])#take the first as product name \n",
    "    price.append(td[i+1])#second as price\n",
    "    instock.append(td[i+2])#third as instock or not \n",
    "\n",
    "x = []\n",
    "for i in range(len(product)):#making them in a list of dicts\n",
    "    x.append({\n",
    "        \"Product\": product[i],\n",
    "        \"price\": price[i],\n",
    "        \"instock\": instock[i]\n",
    "    })\n",
    "# save as csv file\n",
    "with open(\"Prods.csv\", mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Product\", \"price\",\"instock\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(x)\n",
    "print(product)\n",
    "print(price)\n",
    "print(instock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e0ceff97-30ac-4aea-99d4-709a4a08e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sharp Objects', 'In a Dark, Dark Wood', 'The Past Never Ends', 'A Murder in Time']\n",
      "['£47.82', '£19.63', '£56.50', '£16.64']\n",
      "['Add to basket', 'Add to basket', 'Add to basket', 'Add to basket']\n",
      "['Yes', 'Yes', 'Yes', 'No']\n"
     ]
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\",style=\"text-align: center; width: 200px; border: 1px solid #ddd; padding: 10px; border-radius: 5px;\")#get the div with this characteristics\n",
    "Book=[]\n",
    "price=[]\n",
    "stock=[]\n",
    "buttons=[]\n",
    "#getting book name price and stock or not as ordered in list \n",
    "for product in divs:\n",
    "    paragraphs = product.find_all(\"p\")\n",
    "    button=product.find_all(\"button\")\n",
    "    if len(paragraphs) == 3:\n",
    "        Book.append(paragraphs[0].get_text().strip())\n",
    "        price.append(paragraphs[1].get_text().strip())\n",
    "        #stock.append(paragraphs[2].get_text().strip())#if needed for just to stay the same \n",
    "        # print(paragraphs[2].get_text())\n",
    "        if paragraphs[2].get_text().strip().endswith(\"In stock\"):\n",
    "            stock.append(\"Yes\")\n",
    "        else:\n",
    "            stock.append(\"No\")\n",
    "        buttons.append([i.get_text() for i in button][0])\n",
    "print(Book)\n",
    "print(price)\n",
    "print(buttons)\n",
    "print(stock)\n",
    "x = []\n",
    "for i in range(len(Book)):#saving them in a list of dicts\n",
    "    x.append({\n",
    "        \"Book\": Book[i],\n",
    "        \"Price\": price[i],\n",
    "        \"instock\": stock[i],\n",
    "        \"button\":buttons[i]\n",
    "    })\n",
    "with open('Books.json', 'w') as f:\n",
    "    json.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "95339b5d-e3b9-4053-be05-abf3084f28f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'username', 'type': 'text'}, {'name': 'password', 'type': 'password'}, {'name': 'options', 'type': 'select', 'options': ['option1', 'option2', 'option3']}, {'name': 'terms', 'type': 'checkbox', 'default': False}, {'type': 'submit', 'default': 'Submit'}]\n"
     ]
    }
   ],
   "source": [
    "fields = []\n",
    "#get fields names ,types and defaults \n",
    "for tag in soup.find_all(['input', 'select', 'textarea']):\n",
    "    field = {}\n",
    "    name = tag.get('name')#name\n",
    "    if name:\n",
    "        field['name'] = name\n",
    "\n",
    "    if tag.name == 'input':\n",
    "        field['type'] = tag.get('type')#type\n",
    "        if field['type'] in ['checkbox']: #default\n",
    "            field['default'] = tag.has_attr('checked')#if it it is checked\n",
    "        else:\n",
    "            value = tag.get('value')#to know the value that is default\n",
    "            if value:\n",
    "                field['default'] = value\n",
    "    elif tag.name == 'select':\n",
    "        field['type'] = 'select'\n",
    "        options = [option.get('value', '') for option in tag.find_all('option')]#options for select\n",
    "        field['options'] = options\n",
    "\n",
    "    if field:\n",
    "        fields.append(field)\n",
    "\n",
    "print(fields)\n",
    "with open('fields.json', 'w') as f:\n",
    "    json.dump(fields, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a350c005-6750-440c-8dc4-cf72c50bd99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links []\n",
      "links with iframe [{'videolink': ['https://www.youtube.com/watch?v=ujf9RNuBdCU']}]\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "for tag in soup.find_all('a'):#get all a link text and url\n",
    "    link_text = tag.get_text.strip()#linktext\n",
    "    href = tag.get('href')#url\n",
    "    link={link_text:href}#dict\n",
    "    links.append(link)\n",
    "print(\"links\",links)\n",
    "frame = soup.find_all(\"iframe\")#get iframe for the video\n",
    "link=[i.get(\"src\") for i in frame]#get links\n",
    "linkframe={\"videolink\":link}#put in dict\n",
    "links.append(linkframe)\n",
    "print(\"links with iframe\",links)\n",
    "with open('links.json', 'w') as f:\n",
    "    json.dump(links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "78c323b5-14b5-41b9-965f-d0dce639c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '101', 'name': 'Wireless Headphones', 'price': '$49.99', 'colors': 'Black, White, Blue'}, {'id': '102', 'name': 'Smart Speaker', 'price': '$89.99', 'colors': 'Grey, Black'}, {'id': '103', 'name': 'Smart Watch', 'price': '$149.99', 'colors': 'Black, Silver, Gold'}]\n"
     ]
    }
   ],
   "source": [
    "prodcards = soup.find_all(\"div\",class_=\"product-card\")#get all classes as product cards \n",
    "cards=[]\n",
    "for prod in prodcards:#for each one we will have name price and colors and id of the whole product\n",
    "    card={}\n",
    "    card['id'] = prod.get('data-id')#id\n",
    "    paragraphs = prod.find_all(\"p\")#all paragraphs as their order no change\n",
    "    if len(paragraphs) == 3:\n",
    "        card['name']=paragraphs[0].get_text().strip()#first in list is name\n",
    "        card['price']=paragraphs[1].get_text().strip()# second is price\n",
    "        card['colors']=paragraphs[2].get_text().split(\":\")[1].strip() #third is colors\n",
    "    cards.append(card)\n",
    "print(cards)\n",
    "with open('cards.json', 'w') as f:\n",
    "    json.dump(cards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a030f4f-c15f-408e-9e0a-7636daabc9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
